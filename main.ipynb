{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8b851c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading models...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "/var/folders/7p/3ckszphj4s920bw9vwb7wpdw0000gn/T/ipykernel_88282/729718009.py:28: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Models loaded!\n",
      "\n",
      "LangChain is a software that simplifies the development of LLM applications.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from typing import List, Dict\n",
    "import autogen\n",
    "import torch\n",
    "\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "print(\"üöÄ Loading models...\\n\")\n",
    "\n",
    "# Create a Hugging Face text2text pipeline\n",
    "pipe = pipeline(\n",
    "    task=\"text2text-generation\",\n",
    "    model=\"google/flan-t5-base\",\n",
    "    max_length=200,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Wrap it in a LangChain-compatible LLM\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(\"‚úì Models loaded!\\n\")\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Summarize this text: LangChain makes it easy to build LLM applications.\"\n",
    "result = llm.invoke(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "304a0583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 94ca1497-81a3-40fa-bb8d-0f797d647dc4)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-base/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEMO 1: LangChain - Intelligent Prompt Chains\n",
      "======================================================================\n",
      "\n",
      "Task: Create a Python function to calculate Fibonacci sequence\n",
      "\n",
      "LangChain Response:\n",
      "The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The\n",
      "\n",
      "‚úì LangChain demo complete\n",
      "\n",
      "======================================================================\n",
      "DEMO 2: LangChain - Multi-Step Reasoning\n",
      "======================================================================\n",
      "\n",
      "Goal: Build a machine learning model\n",
      "\n",
      "Plan:\n",
      "Step1: Build a machine learning model. Step2: Build a machine learning model.\n",
      "\n",
      "Executing first step...\n",
      "Execution:\n",
      "Collect and prepare data.\n",
      "\n",
      "‚úì Multi-step reasoning complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "# Create a Hugging Face LLM pipeline\n",
    "hf_pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipe)\n",
    "\n",
    "\n",
    "def demo_langchain_basics():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"DEMO 1: LangChain - Intelligent Prompt Chains\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"task\"],\n",
    "        template=\"Task: {task}\\n\\nProvide a detailed step-by-step solution:\"\n",
    "    )\n",
    "\n",
    "    # Use new Runnable API\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    task = \"Create a Python function to calculate Fibonacci sequence\"\n",
    "    print(f\"Task: {task}\\n\")\n",
    "\n",
    "    # Use invoke() instead of run()\n",
    "    result = chain.invoke({\"task\": task})\n",
    "\n",
    "    print(f\"LangChain Response:\\n{result}\\n\")\n",
    "    print(\"‚úì LangChain demo complete\\n\")\n",
    "\n",
    "\n",
    "def demo_langchain_multi_step():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"DEMO 2: LangChain - Multi-Step Reasoning\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "    planner = PromptTemplate(\n",
    "        input_variables=[\"goal\"],\n",
    "        template=\"Break down this goal into 3 steps: {goal}\"\n",
    "    )\n",
    "    executor = PromptTemplate(\n",
    "        input_variables=[\"step\"],\n",
    "        template=\"Explain how to execute this step: {step}\"\n",
    "    )\n",
    "\n",
    "    # New Runnable syntax for each stage\n",
    "    plan_chain = planner | llm | StrOutputParser()\n",
    "    exec_chain = executor | llm | StrOutputParser()\n",
    "\n",
    "    goal = \"Build a machine learning model\"\n",
    "    print(f\"Goal: {goal}\\n\")\n",
    "\n",
    "    # Again, use invoke() not run()\n",
    "    plan = plan_chain.invoke({\"goal\": goal})\n",
    "    print(f\"Plan:\\n{plan}\\n\")\n",
    "\n",
    "    print(\"Executing first step...\")\n",
    "    execution = exec_chain.invoke({\"step\": \"Collect and prepare data\"})\n",
    "    print(f\"Execution:\\n{execution}\\n\")\n",
    "    print(\"‚úì Multi-step reasoning complete\\n\")\n",
    "\n",
    "\n",
    "# Run both demos\n",
    "demo_langchain_basics()\n",
    "demo_langchain_multi_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be8083a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgent:\n",
    "   def __init__(self, name: str, role: str, llm_pipeline):\n",
    "       self.name = name\n",
    "       self.role = role\n",
    "       self.pipe = llm_pipeline\n",
    "       self.memory = []\n",
    "   def process(self, message: str) -> str:\n",
    "       prompt = f\"You are a {self.role}.\\nUser: {message}\\nYour response:\"\n",
    "       response = self.pipe(prompt, max_length=150)[0]['generated_text']\n",
    "       self.memory.append({\"user\": message, \"agent\": response})\n",
    "       return response\n",
    "   def __repr__(self):\n",
    "       return f\"Agent({self.name}, role={self.role})\"\n",
    "\n",
    "\n",
    "def demo_simple_agents():\n",
    "   print(\"=\"*70)\n",
    "   print(\"DEMO 3: Simple Multi-Agent System\")\n",
    "   print(\"=\"*70 + \"\\n\")\n",
    "   researcher = SimpleAgent(\"Researcher\", \"research specialist\", pipe)\n",
    "   coder = SimpleAgent(\"Coder\", \"Python developer\", pipe)\n",
    "   reviewer = SimpleAgent(\"Reviewer\", \"code reviewer\", pipe)\n",
    "   print(\"Agents created:\", researcher, coder, reviewer, \"\\n\")\n",
    "   task = \"Create a function to sort a list\"\n",
    "   print(f\"Task: {task}\\n\")\n",
    "   print(f\"[{researcher.name}] Researching...\")\n",
    "   research = researcher.process(f\"What's the best approach to: {task}\")\n",
    "   print(f\"Research: {research[:100]}...\\n\")\n",
    "   print(f\"[{coder.name}] Coding...\")\n",
    "   code = coder.process(f\"Write Python code to: {task}\")\n",
    "   print(f\"Code: {code[:100]}...\\n\")\n",
    "   print(f\"[{reviewer.name}] Reviewing...\")\n",
    "   review = reviewer.process(f\"Review this approach: {code[:50]}\")\n",
    "   print(f\"Review: {review[:100]}...\\n\")\n",
    "   print(\"‚úì Multi-agent workflow complete\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1e971f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_autogen_conceptual():\n",
    "   print(\"=\"*70)\n",
    "   print(\"DEMO 4: AutoGen Concepts (Conceptual Demo)\")\n",
    "   print(\"=\"*70 + \"\\n\")\n",
    "   agent_config = {\n",
    "       \"agents\": [\n",
    "           {\"name\": \"UserProxy\", \"type\": \"user_proxy\", \"role\": \"Coordinates tasks\"},\n",
    "           {\"name\": \"Assistant\", \"type\": \"assistant\", \"role\": \"Solves problems\"},\n",
    "           {\"name\": \"Executor\", \"type\": \"executor\", \"role\": \"Runs code\"}\n",
    "       ],\n",
    "       \"workflow\": [\n",
    "           \"1. UserProxy receives task\",\n",
    "           \"2. Assistant generates solution\",\n",
    "           \"3. Executor tests solution\",\n",
    "           \"4. Feedback loop until complete\"\n",
    "       ]\n",
    "   }\n",
    "   print(json.dumps(agent_config, indent=2))\n",
    "   print(\"\\nüìù AutoGen Key Features:\")\n",
    "   print(\"  ‚Ä¢ Automated agent chat conversations\")\n",
    "   print(\"  ‚Ä¢ Code execution capabilities\")\n",
    "   print(\"  ‚Ä¢ Human-in-the-loop support\")\n",
    "   print(\"  ‚Ä¢ Multi-agent collaboration\")\n",
    "   print(\"  ‚Ä¢ Tool/function calling\\n\")\n",
    "   print(\"‚úì AutoGen concepts explained\\n\")\n",
    "\n",
    "\n",
    "class MockLLM:\n",
    "   def __init__(self):\n",
    "       self.responses = {\n",
    "           \"code\": \"def fibonacci(n):\\n    if n <= 1:\\n        return n\\n    return fibonacci(n-1) + fibonacci(n-2)\",\n",
    "           \"explain\": \"This is a recursive implementation of the Fibonacci sequence.\",\n",
    "           \"review\": \"The code is correct but could be optimized with memoization.\",\n",
    "           \"default\": \"I understand. Let me help with that task.\"\n",
    "       }\n",
    "   def generate(self, prompt: str) -> str:\n",
    "       prompt_lower = prompt.lower()\n",
    "       if \"code\" in prompt_lower or \"function\" in prompt_lower:\n",
    "           return self.responses[\"code\"]\n",
    "       elif \"explain\" in prompt_lower:\n",
    "           return self.responses[\"explain\"]\n",
    "       elif \"review\" in prompt_lower:\n",
    "           return self.responses[\"review\"]\n",
    "       return self.responses[\"default\"]\n",
    "\n",
    "\n",
    "def demo_autogen_with_mock():\n",
    "   print(\"=\"*70)\n",
    "   print(\"DEMO 5: AutoGen with Custom LLM Backend\")\n",
    "   print(\"=\"*70 + \"\\n\")\n",
    "   mock_llm = MockLLM()\n",
    "   conversation = [\n",
    "       (\"User\", \"Create a fibonacci function\"),\n",
    "       (\"CodeAgent\", mock_llm.generate(\"write code for fibonacci\")),\n",
    "       (\"ReviewAgent\", mock_llm.generate(\"review this code\")),\n",
    "   ]\n",
    "   print(\"Simulated AutoGen Multi-Agent Conversation:\\n\")\n",
    "   for speaker, message in conversation:\n",
    "       print(f\"[{speaker}]\")\n",
    "       print(f\"{message}\\n\")\n",
    "   print(\"‚úì AutoGen simulation complete\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d158806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ü§ñ ADVANCED AGENTIC AI TUTORIAL\n",
      "AutoGen + LangChain + HuggingFace\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "DEMO 1: LangChain - Intelligent Prompt Chains\n",
      "======================================================================\n",
      "\n",
      "Task: Create a Python function to calculate Fibonacci sequence\n",
      "\n",
      "LangChain Response:\n",
      "The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The Fibonacci sequence is a fibonacci sequence. The\n",
      "\n",
      "‚úì LangChain demo complete\n",
      "\n",
      "======================================================================\n",
      "DEMO 2: LangChain - Multi-Step Reasoning\n",
      "======================================================================\n",
      "\n",
      "Goal: Build a machine learning model\n",
      "\n",
      "Plan:\n",
      "Step1: Build a machine learning model. Step2: Build a machine learning model.\n",
      "\n",
      "Executing first step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution:\n",
      "Collect and prepare data.\n",
      "\n",
      "‚úì Multi-step reasoning complete\n",
      "\n",
      "======================================================================\n",
      "DEMO 3: Simple Multi-Agent System\n",
      "======================================================================\n",
      "\n",
      "Agents created: Agent(Researcher, role=research specialist) Agent(Coder, role=Python developer) Agent(Reviewer, role=code reviewer) \n",
      "\n",
      "Task: Create a function to sort a list\n",
      "\n",
      "[Researcher] Researching...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research: To sort a list of items, click on the item you want to sort and then click on the item you want to s...\n",
      "\n",
      "[Coder] Coding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code: a=list(map(int,input().split())) b=list(map(int,input().split())) c=list(map(int,input().split())) d...\n",
      "\n",
      "[Reviewer] Reviewing...\n",
      "Review: a=list(map(int,input().split())))...\n",
      "\n",
      "‚úì Multi-agent workflow complete\n",
      "\n",
      "======================================================================\n",
      "DEMO 4: AutoGen Concepts (Conceptual Demo)\n",
      "======================================================================\n",
      "\n",
      "{\n",
      "  \"agents\": [\n",
      "    {\n",
      "      \"name\": \"UserProxy\",\n",
      "      \"type\": \"user_proxy\",\n",
      "      \"role\": \"Coordinates tasks\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Assistant\",\n",
      "      \"type\": \"assistant\",\n",
      "      \"role\": \"Solves problems\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Executor\",\n",
      "      \"type\": \"executor\",\n",
      "      \"role\": \"Runs code\"\n",
      "    }\n",
      "  ],\n",
      "  \"workflow\": [\n",
      "    \"1. UserProxy receives task\",\n",
      "    \"2. Assistant generates solution\",\n",
      "    \"3. Executor tests solution\",\n",
      "    \"4. Feedback loop until complete\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "üìù AutoGen Key Features:\n",
      "  ‚Ä¢ Automated agent chat conversations\n",
      "  ‚Ä¢ Code execution capabilities\n",
      "  ‚Ä¢ Human-in-the-loop support\n",
      "  ‚Ä¢ Multi-agent collaboration\n",
      "  ‚Ä¢ Tool/function calling\n",
      "\n",
      "‚úì AutoGen concepts explained\n",
      "\n",
      "======================================================================\n",
      "DEMO 5: AutoGen with Custom LLM Backend\n",
      "======================================================================\n",
      "\n",
      "Simulated AutoGen Multi-Agent Conversation:\n",
      "\n",
      "[User]\n",
      "Create a fibonacci function\n",
      "\n",
      "[CodeAgent]\n",
      "def fibonacci(n):\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "[ReviewAgent]\n",
      "def fibonacci(n):\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "‚úì AutoGen simulation complete\n",
      "\n",
      "======================================================================\n",
      "DEMO 6: Hybrid LangChain + Multi-Agent System\n",
      "======================================================================\n",
      "\n",
      "Problem: Optimize a slow database query\n",
      "\n",
      "[LangChain] Analyzing problem...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis: The first step is to optimize the database query. The second step is to optimize the database query. The third step is t...\n",
      "\n",
      "[Planner] Creating plan...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan: Optimize a slow database query...\n",
      "\n",
      "[Executor] Executing...\n",
      "Result: Add database indexes...\n",
      "\n",
      "‚úì Hybrid system complete\n",
      "\n",
      "======================================================================\n",
      "üéâ TUTORIAL COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "üìö What You Learned:\n",
      "  ‚úì LangChain prompt engineering and chains\n",
      "  ‚úì Multi-step reasoning with LangChain\n",
      "  ‚úì Building custom multi-agent systems\n",
      "  ‚úì AutoGen architecture and concepts\n",
      "  ‚úì Combining LangChain + agents\n",
      "  ‚úì Using HuggingFace models (no API needed!)\n",
      "\n",
      "üí° Key Takeaway:\n",
      "  You can build powerful agentic AI systems without expensive APIs!\n",
      "  Combine LangChain's chains with multi-agent architectures for\n",
      "  intelligent, autonomous AI systems.\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def demo_hybrid_system():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"DEMO 6: Hybrid LangChain + Multi-Agent System\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "    reasoning_prompt = PromptTemplate(\n",
    "        input_variables=[\"problem\"],\n",
    "        template=\"Analyze this problem: {problem}\\nWhat are the key steps?\"\n",
    "    )\n",
    "\n",
    "    # Updated: Runnable chain\n",
    "    reasoning_chain = reasoning_prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Example SimpleAgent class usage\n",
    "    planner = SimpleAgent(\"Planner\", \"strategic planner\", pipe)\n",
    "    executor = SimpleAgent(\"Executor\", \"task executor\", pipe)\n",
    "\n",
    "    problem = \"Optimize a slow database query\"\n",
    "    print(f\"Problem: {problem}\\n\")\n",
    "\n",
    "    print(\"[LangChain] Analyzing problem...\")\n",
    "\n",
    "    # ‚úÖ Use .invoke() instead of .run()\n",
    "    analysis = reasoning_chain.invoke({\"problem\": problem})\n",
    "    print(f\"Analysis: {analysis[:120]}...\\n\")\n",
    "\n",
    "    print(f\"[{planner.name}] Creating plan...\")\n",
    "    plan = planner.process(f\"Plan how to: {problem}\")\n",
    "    print(f\"Plan: {plan[:120]}...\\n\")\n",
    "\n",
    "    print(f\"[{executor.name}] Executing...\")\n",
    "    result = executor.process(\"Execute: Add database indexes\")\n",
    "    print(f\"Result: {result[:120]}...\\n\")\n",
    "\n",
    "    print(\"‚úì Hybrid system complete\\n\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   print(\"=\"*70)\n",
    "   print(\"ü§ñ ADVANCED AGENTIC AI TUTORIAL\")\n",
    "   print(\"AutoGen + LangChain + HuggingFace\")\n",
    "   print(\"=\"*70 + \"\\n\")\n",
    "   demo_langchain_basics()\n",
    "   demo_langchain_multi_step()\n",
    "   demo_simple_agents()\n",
    "   demo_autogen_conceptual()\n",
    "   demo_autogen_with_mock()\n",
    "   demo_hybrid_system()\n",
    "   print(\"=\"*70)\n",
    "   print(\"üéâ TUTORIAL COMPLETE!\")\n",
    "   print(\"=\"*70)\n",
    "   print(\"\\nüìö What You Learned:\")\n",
    "   print(\"  ‚úì LangChain prompt engineering and chains\")\n",
    "   print(\"  ‚úì Multi-step reasoning with LangChain\")\n",
    "   print(\"  ‚úì Building custom multi-agent systems\")\n",
    "   print(\"  ‚úì AutoGen architecture and concepts\")\n",
    "   print(\"  ‚úì Combining LangChain + agents\")\n",
    "   print(\"  ‚úì Using HuggingFace models (no API needed!)\")\n",
    "   print(\"\\nüí° Key Takeaway:\")\n",
    "   print(\"  You can build powerful agentic AI systems without expensive APIs!\")\n",
    "   print(\"  Combine LangChain's chains with multi-agent architectures for\")\n",
    "   print(\"  intelligent, autonomous AI systems.\")\n",
    "   print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360df310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
